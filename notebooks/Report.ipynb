{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **DATA AND INFORMATION QUALITY**\n",
        "## **Report: Milan Public Establishments Dataset Analysis**\n",
        "\n",
        "---\n",
        "\n",
        "**Authors:** Data Quality Team  \n",
        "**Date:** January 2026  \n",
        "**Dataset:** Comune di Milano - Pubblici Esercizi\n",
        "\n",
        "---\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "1. [Introduction](#1-introduction)\n",
        "2. [Setup Choices](#2-setup-choices)\n",
        "3. [Pipeline Implementation](#3-pipeline-implementation)\n",
        "4. [After Cleaning](#4-after-cleaning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# 1. INTRODUCTION\n",
        "\n",
        "This report documents the complete **Data Quality Assessment and Cleaning Pipeline** applied to the Milan Public Establishments dataset (*Comune di Milano - Pubblici Esercizi*).\n",
        "\n",
        "## 1.1 Dataset Description\n",
        "\n",
        "The dataset contains information about **public establishments** (bars, restaurants, shops, etc.) registered in the Municipality of Milan. Each record represents a business with attributes including:\n",
        "\n",
        "- **Location data:** Street address, civic number, zone code (ZD)\n",
        "- **Business type:** Sector, exercise type, commercial form\n",
        "- **Physical attributes:** Surface area for food service\n",
        "- **Business name:** Sign/brand name (Insegna)\n",
        "\n",
        "## 1.2 Objectives\n",
        "\n",
        "1. **Profile** the dataset to understand its structure and characteristics\n",
        "2. **Assess** data quality dimensions (Completeness, Consistency, Duplicates)\n",
        "3. **Clean** the data through transformation, error correction, and deduplication\n",
        "4. **Validate** the improvements through post-cleaning profiling\n",
        "\n",
        "## 1.3 Data Quality Dimensions Covered\n",
        "\n",
        "| Dimension | Covered | Rationale |\n",
        "|-----------|---------|----------|\n",
        "| **Completeness** | ‚úÖ Yes | Measured and improved through imputation |\n",
        "| **Consistency** | ‚úÖ Yes | Address consistency check, functional dependencies |\n",
        "| **Duplicates** | ‚úÖ Yes | Exact and near-duplicate detection |\n",
        "| **Accuracy** | ‚ùå No | No ground truth available for validation |\n",
        "| **Timeliness** | ‚ùå No | No temporal attributes (dates) in dataset |\n",
        "\n",
        "### Why No Accuracy Assessment?\n",
        "\n",
        "**Accuracy** measures how well data values correspond to the real-world entities they represent. To assess accuracy, we need either:\n",
        "- A **ground truth** dataset to compare against\n",
        "- **External validation sources** (e.g., official registry, field verification)\n",
        "\n",
        "Since we lack both, we **cannot objectively measure accuracy**. We can only ensure **syntactic correctness** and **internal consistency**.\n",
        "\n",
        "### Why No Timeliness Assessment?\n",
        "\n",
        "**Timeliness** measures whether data is up-to-date for the intended use. This dataset lacks:\n",
        "- Timestamp columns (creation/update dates)\n",
        "- Temporal attributes to assess currency\n",
        "\n",
        "Therefore, **timeliness cannot be measured**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# 2. SETUP CHOICES\n",
        "\n",
        "## 2.1 Environment\n",
        "\n",
        "| Component | Version/Details |\n",
        "|-----------|----------------|\n",
        "| **Operating System** | Linux (Ubuntu) |\n",
        "| **Python** | 3.12 |\n",
        "| **IDE** | Visual Studio Code with Jupyter extension |\n",
        "| **Kernel** | IPython Kernel |\n",
        "\n",
        "## 2.2 Libraries and Tools\n",
        "\n",
        "### Core Data Processing\n",
        "| Library | Purpose |\n",
        "|---------|---------|\n",
        "| `pandas` | DataFrame manipulation and analysis |\n",
        "| `numpy` | Numerical operations |\n",
        "\n",
        "### Data Profiling\n",
        "| Library | Purpose |\n",
        "|---------|---------|\n",
        "| `ydata_profiling` | Automated profiling reports (HTML/JSON) |\n",
        "\n",
        "### Visualization\n",
        "| Library | Purpose |\n",
        "|---------|---------|\n",
        "| `matplotlib` | Basic plots (histograms, boxplots) |\n",
        "| `seaborn` | Statistical visualizations (heatmaps) |\n",
        "\n",
        "### Statistical Analysis\n",
        "| Library | Purpose |\n",
        "|---------|---------|\n",
        "| `scipy.stats` | Z-score calculations for outlier detection |\n",
        "\n",
        "### Functional Dependencies (Custom Scripts)\n",
        "| Script | Purpose |\n",
        "|--------|---------|\n",
        "| `tane.py` | TANE algorithm implementation |\n",
        "| `ctane.py` | Conditional TANE |\n",
        "| `fdtool.py` | FD_Mine implementation |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import all required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "import re\n",
        "\n",
        "# Display settings\n",
        "pd.set_option('display.max_columns', 50)\n",
        "pd.set_option('display.width', 140)\n",
        "%matplotlib inline\n",
        "\n",
        "print(\"‚úÖ Libraries loaded successfully!\")\n",
        "print(f\"   Pandas version: {pd.__version__}\")\n",
        "print(f\"   NumPy version: {np.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# 3. PIPELINE IMPLEMENTATION\n",
        "\n",
        "## 3.1 Exploration\n",
        "\n",
        "### Load the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the original dataset\n",
        "MILANO = pd.read_csv(\"Comune-di-Milano-Pubblici-esercizi(in)-2.csv\", sep=\";\")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"DATASET OVERVIEW\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nüìä Shape: {MILANO.shape[0]:,} rows √ó {MILANO.shape[1]} columns\")\n",
        "print(f\"üì¶ Total cells: {MILANO.shape[0] * MILANO.shape[1]:,}\")\n",
        "\n",
        "# Preview the data\n",
        "print(\"\\nüìã First 5 rows:\")\n",
        "MILANO.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.2 Data Profiling\n",
        "\n",
        "### 3.2.1 Profiling Formulas\n",
        "\n",
        "Let $N$ be the total number of rows, $n$ the count of non-null values, and $d$ the number of distinct values.\n",
        "\n",
        "| Metric | Formula | Description |\n",
        "|--------|---------|-------------|\n",
        "| **Count** | $n = \\sum_{i=1}^{N} \\mathbb{1}[x_i \\neq \\text{null}]$ | Number of non-null values |\n",
        "| **Distinct** | $d = |\\{x_i : x_i \\neq \\text{null}\\}|$ | Number of unique values |\n",
        "| **Uniqueness** | $U = \\frac{d}{N}$ | Ratio of distinct to total rows |\n",
        "| **Distinctness** | $D = \\frac{d}{n}$ | Ratio of distinct to non-null values |\n",
        "| **Constancy** | $C = \\frac{\\max(\\text{freq})}{n}$ | Frequency of most common value |\n",
        "| **Null Ratio** | $\\text{NR} = \\frac{N - n}{N}$ | Proportion of missing values |\n",
        "\n",
        "#### Key Profiling Concepts\n",
        "\n",
        "- **Uniqueness = 1.0**: Every row has a distinct value (potential key)\n",
        "- **Constancy ‚âà 1.0**: Almost all values are the same (low information)\n",
        "- **Distinctness = 1.0**: No duplicate values among non-null entries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute profiling metrics for all columns\n",
        "ROWS = len(MILANO)\n",
        "\n",
        "profile_data = []\n",
        "for col in MILANO.columns:\n",
        "    count = MILANO[col].count()\n",
        "    distinct = MILANO[col].nunique()\n",
        "    uniqueness = distinct / ROWS\n",
        "    distinctness = distinct / count if count > 0 else 0\n",
        "    mode_freq = MILANO[col].value_counts().iloc[0] if count > 0 else 0\n",
        "    constancy = mode_freq / count if count > 0 else 0\n",
        "    null_ratio = (ROWS - count) / ROWS\n",
        "    \n",
        "    profile_data.append({\n",
        "        'Column': col,\n",
        "        'Count (n)': count,\n",
        "        'Nulls': ROWS - count,\n",
        "        'Null Ratio': round(null_ratio, 4),\n",
        "        'Distinct (d)': distinct,\n",
        "        'Uniqueness (d/N)': round(uniqueness, 4),\n",
        "        'Distinctness (d/n)': round(distinctness, 4),\n",
        "        'Constancy': round(constancy, 4)\n",
        "    })\n",
        "\n",
        "profile_df = pd.DataFrame(profile_data)\n",
        "print(\"üìä PROFILING METRICS FOR ALL COLUMNS\")\n",
        "profile_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2.2 Automatic Profiling with YData Profiling\n",
        "\n",
        "YData Profiling generates a comprehensive report including:\n",
        "- **Overview**: Dataset statistics, variable types, missing values\n",
        "- **Variables**: Detailed analysis per column\n",
        "- **Interactions**: Correlations and relationships\n",
        "- **Missing Values**: Patterns and heatmaps\n",
        "- **Duplicates**: Exact duplicate detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ydata_profiling import ProfileReport\n",
        "\n",
        "# Generate profiling report for original dataset\n",
        "PROFILE_ORIGINAL = ProfileReport(\n",
        "    MILANO, \n",
        "    title=\"Profiling Report - Milan Public Establishments (ORIGINAL)\",\n",
        "    explorative=True\n",
        ")\n",
        "\n",
        "# Display inline\n",
        "PROFILE_ORIGINAL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2.3 Completeness Assessment\n",
        "\n",
        "**Completeness** measures the degree to which all required data is present.\n",
        "\n",
        "$$\\text{Completeness} = \\frac{\\text{Non-null cells}}{\\text{Total cells}} = \\frac{\\sum_{i,j} \\mathbb{1}[x_{ij} \\neq \\text{null}]}{N \\times M}$$\n",
        "\n",
        "Where $N$ is the number of rows and $M$ is the number of columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate overall completeness\n",
        "TOTAL_CELLS = MILANO.shape[0] * MILANO.shape[1]\n",
        "NON_NULL_CELLS = MILANO.count().sum()\n",
        "NULL_CELLS = MILANO.isnull().sum().sum()\n",
        "\n",
        "COMPLETENESS = NON_NULL_CELLS / TOTAL_CELLS\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"COMPLETENESS ASSESSMENT\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nüìä Total cells: {TOTAL_CELLS:,}\")\n",
        "print(f\"‚úÖ Non-null cells: {NON_NULL_CELLS:,}\")\n",
        "print(f\"‚ùå Null cells: {NULL_CELLS:,}\")\n",
        "print(f\"\\nüìà Overall Completeness: {COMPLETENESS*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Completeness per column\n",
        "null_counts = MILANO.isnull().sum()\n",
        "null_pct = (null_counts / len(MILANO) * 100).round(2)\n",
        "\n",
        "missing_df = pd.DataFrame({\n",
        "    'Column': null_counts.index,\n",
        "    'Missing Count': null_counts.values,\n",
        "    'Missing %': null_pct.values,\n",
        "    'Completeness %': (100 - null_pct.values).round(2)\n",
        "}).sort_values('Missing %', ascending=False)\n",
        "\n",
        "print(\"\\nüìã MISSING VALUES BY COLUMN:\")\n",
        "missing_df[missing_df['Missing Count'] > 0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2.4 Consistency Assessment\n",
        "\n",
        "**Consistency** measures whether data values conform to defined rules and constraints.\n",
        "\n",
        "We check two types of consistency:\n",
        "\n",
        "#### Type 1: Value-based Consistency\n",
        "- `Superficie somministrazione` should be > 0 when present\n",
        "\n",
        "$$\\text{Consistency}_{\\text{rule}} = \\frac{|\\{x : \\text{rule}(x) = \\text{True}\\}|}{n}$$\n",
        "\n",
        "#### Type 2: Functional Dependencies (FD)\n",
        "- Address consistency: `Codice via ‚Üí Nome via, Tipo via`\n",
        "- Zone consistency: Cross-validation between `Indirizzo` and structured fields"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Value-based consistency: Superficie > 0\n",
        "MILANO[\"Superficie somministrazione\"] = pd.to_numeric(\n",
        "    MILANO[\"Superficie somministrazione\"], errors=\"coerce\"\n",
        ")\n",
        "\n",
        "valid_superficie = MILANO[\"Superficie somministrazione\"].notna()\n",
        "positive_superficie = MILANO[\"Superficie somministrazione\"] > 0\n",
        "\n",
        "consistency_superficie = (valid_superficie & positive_superficie).sum() / valid_superficie.sum()\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"CONSISTENCY ASSESSMENT\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nüìä Rule: Superficie somministrazione > 0\")\n",
        "print(f\"‚úÖ Valid (non-null) records: {valid_superficie.sum():,}\")\n",
        "print(f\"‚úÖ Positive values: {positive_superficie.sum():,}\")\n",
        "print(f\"\\nüìà Consistency: {consistency_superficie*100:.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2.5 Functional Dependencies (FD)\n",
        "\n",
        "A **Functional Dependency** $X \\rightarrow Y$ holds if, whenever two rows agree on attribute(s) $X$, they must also agree on attribute $Y$.\n",
        "\n",
        "$$X \\rightarrow Y \\Leftrightarrow \\forall r_1, r_2 \\in R: r_1[X] = r_2[X] \\Rightarrow r_1[Y] = r_2[Y]$$\n",
        "\n",
        "#### Expected FDs in Address Data:\n",
        "- `Codice via ‚Üí Nome via` (street code determines street name)\n",
        "- `Codice via ‚Üí Tipo via` (street code determines street type)\n",
        "\n",
        "We use algorithms like **TANE** and **FD_Mine** to discover and validate FDs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def check_fd(df, lhs_cols, rhs_col):\n",
        "    \"\"\"Check if functional dependency X -> Y holds.\"\"\"\n",
        "    if isinstance(lhs_cols, str):\n",
        "        lhs_cols = [lhs_cols]\n",
        "    \n",
        "    # Group by LHS and count distinct RHS values\n",
        "    grouped = df.groupby(lhs_cols)[rhs_col].nunique()\n",
        "    violations = grouped[grouped > 1]\n",
        "    \n",
        "    total_groups = len(grouped)\n",
        "    violating_groups = len(violations)\n",
        "    \n",
        "    if violating_groups == 0:\n",
        "        status = \"‚úÖ HOLDS\"\n",
        "    else:\n",
        "        status = f\"‚ùå VIOLATED in {violating_groups}/{total_groups} groups\"\n",
        "    \n",
        "    print(f\"FD: {lhs_cols} ‚Üí {rhs_col}: {status}\")\n",
        "    return violating_groups == 0\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"FUNCTIONAL DEPENDENCY CHECK\")\n",
        "print(\"=\" * 60)\n",
        "print()\n",
        "\n",
        "# Check expected FDs\n",
        "check_fd(MILANO, 'Codice via', 'Descrizione via')\n",
        "check_fd(MILANO, 'Codice via', 'Tipo via')\n",
        "check_fd(MILANO, ['Codice via', 'Civico'], 'ZD')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2.6 Duplicate Detection\n",
        "\n",
        "**Duplicates** are rows that represent the same real-world entity.\n",
        "\n",
        "We distinguish:\n",
        "- **Exact duplicates**: Rows identical across all columns\n",
        "- **Near-duplicates**: Rows with minor differences (typos, formatting)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for exact duplicates\n",
        "exact_duplicates = MILANO.duplicated().sum()\n",
        "all_duplicates = MILANO.duplicated(keep=False).sum()\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"DUPLICATE DETECTION\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nüìä Total rows: {len(MILANO):,}\")\n",
        "print(f\"üîÑ Exact duplicate rows: {exact_duplicates}\")\n",
        "print(f\"üîÑ Total rows involved in duplication: {all_duplicates}\")\n",
        "\n",
        "if exact_duplicates > 0:\n",
        "    print(\"\\n‚ö†Ô∏è Duplicate rows found!\")\n",
        "else:\n",
        "    print(\"\\n‚úÖ No exact duplicates found.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 3.3 Data Cleaning\n",
        "\n",
        "The cleaning pipeline consists of three main phases:\n",
        "\n",
        "1. **Data Transformation/Standardization** - Normalize formats, fix encoding\n",
        "2. **Error Detection and Correction** - Handle missing values, repair inconsistencies\n",
        "3. **Data Deduplication** - Remove redundant records\n",
        "\n",
        "### 3.3.1 Data Transformation/Standardization\n",
        "\n",
        "| Operation | Description | Example |\n",
        "|-----------|-------------|--------|\n",
        "| Text Normalization | Convert to lowercase | `BAR MILANO` ‚Üí `bar milano` |\n",
        "| Column Renaming | Fix encoding issues | `√æ√øTipo...` ‚Üí `Tipo esercizio...` |\n",
        "| Typo Correction | Fix special characters | `caff√ø` ‚Üí `caff√®` |\n",
        "| Macro-Category Creation | Group similar business types | `BAR CAFFE, BIRRERIA` ‚Üí `BAR` |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === TEXT NORMALIZATION ===\n",
        "print(\"=\" * 60)\n",
        "print(\"STEP 1: TEXT NORMALIZATION\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Convert text columns to lowercase\n",
        "text_cols = MILANO.select_dtypes(include=\"object\").columns\n",
        "MILANO[text_cols] = MILANO[text_cols].apply(lambda col: col.str.lower())\n",
        "print(\"\\n‚úÖ Converted all text columns to lowercase\")\n",
        "\n",
        "# Rename problematic columns\n",
        "col_renames = {\n",
        "    \"√æ√øTipo esercizio storico pe\": \"Tipo esercizio storico pubblico esercizio\",\n",
        "    \"Ubicazione\": \"Indirizzo\",\n",
        "    \"Descrizione via\": \"Nome via\",\n",
        "    \"Forma commercio prev\": \"Forma commercio precedente\",\n",
        "    \"Settore storico pe\": \"Settore storico pubblico esercizio\"\n",
        "}\n",
        "existing_cols = {k: v for k, v in col_renames.items() if k in MILANO.columns}\n",
        "MILANO = MILANO.rename(columns=existing_cols)\n",
        "print(f\"‚úÖ Renamed {len(existing_cols)} columns with encoding issues\")\n",
        "\n",
        "# Fix caff√® pattern\n",
        "text_cols = MILANO.select_dtypes(include=\"object\").columns\n",
        "MILANO[text_cols] = MILANO[text_cols].apply(\n",
        "    lambda col: col.str.replace(r\"\\bcaff[√ø√Ω]\", \"caff√®\", regex=True)\n",
        ")\n",
        "print(\"‚úÖ Fixed 'caff√ø' ‚Üí 'caff√®' pattern\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3.2 Error Detection and Correction\n",
        "\n",
        "#### Missing Values Imputation Strategies\n",
        "\n",
        "| Column | Strategy | Rationale |\n",
        "|--------|----------|----------|\n",
        "| `Insegna` | Fill with \"unknown\" | Cannot infer business names |\n",
        "| `Superficie` | KNN by street/zone, then global mean | Nearby establishments likely similar |\n",
        "| `Forma commercio prev` | Mode by macro-category (conf ‚â• 80%) | Business type determines commerce form |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === MISSING VALUES IMPUTATION ===\n",
        "print(\"=\" * 60)\n",
        "print(\"STEP 2: MISSING VALUES IMPUTATION\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Track original missing counts\n",
        "missing_before = MILANO.isnull().sum().sum()\n",
        "print(f\"\\nüìä Missing values BEFORE: {missing_before:,}\")\n",
        "\n",
        "# 1. Fill Insegna with 'unknown'\n",
        "if 'Insegna' in MILANO.columns:\n",
        "    insegna_missing = MILANO['Insegna'].isna().sum()\n",
        "    MILANO['Insegna'] = MILANO['Insegna'].fillna('unknown')\n",
        "    print(f\"\\n‚úÖ Insegna: Filled {insegna_missing} missing values with 'unknown'\")\n",
        "\n",
        "# 2. Fill Superficie using KNN-like approach (same street, then zone, then global mean)\n",
        "sup_col = 'Superficie somministrazione'\n",
        "if sup_col in MILANO.columns:\n",
        "    MILANO[sup_col] = pd.to_numeric(MILANO[sup_col], errors='coerce')\n",
        "    sup_missing_before = MILANO[sup_col].isna().sum()\n",
        "    \n",
        "    # Strategy 1: Same street mean\n",
        "    via_means = MILANO.groupby('Codice via')[sup_col].transform('mean')\n",
        "    mask_via = MILANO[sup_col].isna() & via_means.notna()\n",
        "    MILANO.loc[mask_via, sup_col] = via_means[mask_via]\n",
        "    \n",
        "    # Strategy 2: Same zone mean\n",
        "    zd_means = MILANO.groupby('ZD')[sup_col].transform('mean')\n",
        "    mask_zd = MILANO[sup_col].isna() & zd_means.notna()\n",
        "    MILANO.loc[mask_zd, sup_col] = zd_means[mask_zd]\n",
        "    \n",
        "    # Strategy 3: Global mean\n",
        "    global_mean = MILANO[sup_col].mean()\n",
        "    MILANO[sup_col] = MILANO[sup_col].fillna(global_mean)\n",
        "    \n",
        "    print(f\"‚úÖ {sup_col}: Filled {sup_missing_before} missing values using KNN + global mean\")\n",
        "\n",
        "# Summary\n",
        "missing_after = MILANO.isnull().sum().sum()\n",
        "print(f\"\\nüìä Missing values AFTER: {missing_after:,}\")\n",
        "print(f\"üìà Improvement: {missing_before - missing_after:,} cells filled\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3.3 Data Deduplication\n",
        "\n",
        "#### Similarity Measures Used\n",
        "\n",
        "**Jaccard Similarity** (set-based):\n",
        "$$J(A, B) = \\frac{|A \\cap B|}{|A \\cup B|}$$\n",
        "\n",
        "**Levenshtein Similarity** (edit distance-based):\n",
        "$$L_{sim}(s_1, s_2) = 1 - \\frac{\\text{editDistance}(s_1, s_2)}{\\max(|s_1|, |s_2|)}$$\n",
        "\n",
        "#### Blocking Strategy\n",
        "To avoid $O(n^2)$ comparisons, we use **blocking**:\n",
        "- Group records by `(Codice via, Civico)` \n",
        "- Only compare records within the same block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === EXACT DUPLICATE REMOVAL ===\n",
        "print(\"=\" * 60)\n",
        "print(\"STEP 3: DATA DEDUPLICATION\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "before_count = len(MILANO)\n",
        "MILANO = MILANO.drop_duplicates(keep='first')\n",
        "after_count = len(MILANO)\n",
        "\n",
        "print(f\"\\nüìä Rows before: {before_count:,}\")\n",
        "print(f\"üìä Rows after: {after_count:,}\")\n",
        "print(f\"üóëÔ∏è Exact duplicates removed: {before_count - after_count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define similarity functions\n",
        "def levenshtein_distance(s1, s2):\n",
        "    \"\"\"Compute Levenshtein (edit) distance.\"\"\"\n",
        "    if pd.isna(s1) or pd.isna(s2):\n",
        "        return float('inf')\n",
        "    s1, s2 = str(s1).lower(), str(s2).lower()\n",
        "    if len(s1) < len(s2):\n",
        "        s1, s2 = s2, s1\n",
        "    if len(s2) == 0:\n",
        "        return len(s1)\n",
        "    previous_row = range(len(s2) + 1)\n",
        "    for i, c1 in enumerate(s1):\n",
        "        current_row = [i + 1]\n",
        "        for j, c2 in enumerate(s2):\n",
        "            insertions = previous_row[j + 1] + 1\n",
        "            deletions = current_row[j] + 1\n",
        "            substitutions = previous_row[j] + (c1 != c2)\n",
        "            current_row.append(min(insertions, deletions, substitutions))\n",
        "        previous_row = current_row\n",
        "    return previous_row[-1]\n",
        "\n",
        "def levenshtein_similarity(s1, s2):\n",
        "    \"\"\"Compute normalized Levenshtein similarity (0-1).\"\"\"\n",
        "    if pd.isna(s1) or pd.isna(s2):\n",
        "        return 0.0\n",
        "    s1, s2 = str(s1), str(s2)\n",
        "    max_len = max(len(s1), len(s2))\n",
        "    if max_len == 0:\n",
        "        return 1.0\n",
        "    return 1 - (levenshtein_distance(s1, s2) / max_len)\n",
        "\n",
        "print(\"‚úÖ Similarity functions defined\")\n",
        "print(f\"   Example: levenshtein_similarity('caff√®', 'caffe') = {levenshtein_similarity('caff√®', 'caffe'):.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# 4. AFTER CLEANING\n",
        "\n",
        "## 4.1 Final Data Profiling\n",
        "\n",
        "After completing all cleaning steps, we perform a final profiling to assess improvements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate final profiling report\n",
        "PROFILE_CLEANED = ProfileReport(\n",
        "    MILANO, \n",
        "    title=\"Profiling Report - Milan Public Establishments (CLEANED)\",\n",
        "    explorative=True\n",
        ")\n",
        "\n",
        "# Display inline\n",
        "PROFILE_CLEANED"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.2 Before vs After Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load original for comparison\n",
        "MILANO_ORIG = pd.read_csv(\"Comune-di-Milano-Pubblici-esercizi(in)-2.csv\", sep=\";\")\n",
        "\n",
        "# Calculate metrics\n",
        "orig_cells = MILANO_ORIG.shape[0] * MILANO_ORIG.shape[1]\n",
        "clean_cells = MILANO.shape[0] * MILANO.shape[1]\n",
        "orig_missing = MILANO_ORIG.isnull().sum().sum()\n",
        "clean_missing = MILANO.isnull().sum().sum()\n",
        "\n",
        "comparison = pd.DataFrame({\n",
        "    'Metric': [\n",
        "        'Total Rows',\n",
        "        'Total Columns',\n",
        "        'Total Cells',\n",
        "        'Missing Cells',\n",
        "        'Completeness %'\n",
        "    ],\n",
        "    'Original': [\n",
        "        f\"{MILANO_ORIG.shape[0]:,}\",\n",
        "        MILANO_ORIG.shape[1],\n",
        "        f\"{orig_cells:,}\",\n",
        "        f\"{orig_missing:,}\",\n",
        "        f\"{(1 - orig_missing/orig_cells)*100:.2f}%\"\n",
        "    ],\n",
        "    'Cleaned': [\n",
        "        f\"{MILANO.shape[0]:,}\",\n",
        "        MILANO.shape[1],\n",
        "        f\"{clean_cells:,}\",\n",
        "        f\"{clean_missing:,}\",\n",
        "        f\"{(1 - clean_missing/clean_cells)*100:.2f}%\"\n",
        "    ]\n",
        "})\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"BEFORE VS AFTER COMPARISON\")\n",
        "print(\"=\" * 60)\n",
        "comparison"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
